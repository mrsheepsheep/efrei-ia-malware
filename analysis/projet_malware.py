# -*- coding: utf-8 -*-
"""Projet Malware

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rd7etX_f4CJH3Lja266ydWJQ4luPjkQA

Lire les données de ce dataset au format CSV. Les colonnes (observations ou features pour reprendre le jargon machine learning) de chaque ligne de ce fichier CSV correspondent aux caractéristiques d'un exécutable windows (taille de l'executable, nombre de sections, l'entropie, etc). La dernière colonne (legitimate) nous informe si l'exécutable est un malware ou non (c'est l'étiquette ou label pour reprendre le jargon machine learning)
*
 Comprendre la structure du dataset
* Création de l'ensemble d'entrainement et de test
* Analyse de la corrélation des données
* Extraction des étiquettes
* Nettoyage du dataset
* Feature Scaling

# Chargement du dataset
"""

import pandas as pd
import numpy as np
import pickle
import sklearn.ensemble as ske
from sklearn import metrics
from sklearn import tree, linear_model
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.externals import joblib
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

#Import du dataset du sujet
dataset = "https://raw.githubusercontent.com/securitylab-repository/malware_classification/master/datasets/malware-detection/malwaredata.csv"
malware = pd.read_csv(dataset, delimiter=',')

#Import du dataset de la partie 2, créé par nous dans le cadre du projet
dataset_Partie2 = "https://raw.githubusercontent.com/mrsheepsheep/TPS-IA/master/dataset.csv"
malware_Partie2 = pd.read_csv(dataset_Partie2, delimiter=',')

#commenter lorsque l'on veut ignorer le dataset de la partie 2
malware = pd.concat([malware,malware_Partie2])

# Suppression de la dernière colonne vide
malware = malware.drop(columns='Unnamed: 57')

# Suppression des lignes contenant des null
malware = malware.dropna()

"""# Structure des données
* Sample
* Informations sur les colonnes

## Sample
"""

malware.sample()

malware_Partie2.sample()

"""## Info"""

malware.info()

"""On remarque l'existence de deux champs `object`: `md5` et `Machine`
* Le numéro de machine associé ne pourrait être utile que si 

Vérifions quand même le nombre de valeurs de ces champs.
"""

malware["md5"].value_counts()

malware["Machine"].value_counts()

"""**Remarque**: il semble qu'un md5 se soit faufilé dans la colonne Machine. Après recherche de la ligne concernée, nous avons remarqué que le nom du fichier (pdf.exe) s'était inséré avant le md5, créant alors une colonne supplémentaire. Pour régler le problème, nous allons supprimer cette ligne."""

malware = malware[malware['Machine'] != '3ab1aa9785d0681434766bb0ffc4a13c']
malware["Machine"].value_counts()

"""## Données statistiques du dataset"""

malware.describe()

"""Il est intéressant de noter que c'est la colonne `legitimate` qui nous permet de savoir si c'est effectivement un malware ou non."""

malware['legitimate'].describe()

"""On a une répartition d'environ 34% de fichiers légitimes. Il pourrait être intéressant de séparer ces deux groupes pour en comprendre les caractéristiques séparées.

### Fichiers malveillants
"""

files_malware = malware[malware['legitimate'] == 0]
files_malware.describe()

"""### Fichiers légitimes"""

files_legitimate = malware[malware['legitimate'] == 1]
files_legitimate.describe()

"""### Affichage en histogramme
Pour déterminer quels attributs pourraient nous intéresser, on compare les données des deux catégories de fichiers.
"""

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

files_legitimate.hist()
plt.show();

"""# Selection des attributs les plus représentatifs

Nous avons dans notre dataset 57 colonnes différentes qui nous donnent des informations relatives aux differents fichiers. Cependant, toutes ces informations ne sont pas forcement indispensables pour isoler quel fichier est malveillant et lequel ne n'est pas. 
Dans un but de simplification des simulations, nous pouvons (et devons) donc réduire ce dataset à un ensemble sur lequel il sera plus simple de travailler.
Tout le code de cette section se résumera donc à la recherche et à l'isolation de ces informations importantes.

En ennumerant la liste de toutes les informations que l'on possède, on peut commencer à isoler individuellement des colonnes qui peuvent être utiles. Nous allons donc chercher à taton quels colonnes peuvent faire le plus sens dans notre analyse.

Nous commencons par la colonne "FileAlignment"
"""

files_legitimate['FileAlignment'].value_counts()

files_malware['FileAlignment'].value_counts()

"""On remarque donc ici que les résultats ne sont pas spécialement probants et ne nous aident pas à isoler les colonnes interessantes. Certaines colonnes seront donc clairement inutiles pour notre analyse.

De plus, tester à la main les 57 possibilités serait vraiment trop long.

D'un autre côté, certaines valeurs ont l'air prometteuses telles que "SectionMaxEntropy" qui est l'entropie maximale des differentes sections. Nous avons eu l'idée d'utiliser cette valeur car elle est indiquée dans la partie 2 du projet. Si cette valeur est indiquée expréssément, c'est qu'elle doit avoir une certaine importance dans l'analyse.
"""

plt.hist([files_legitimate['SectionsMeanEntropy'], files_malware['SectionsMeanEntropy']],color=["green", "blue"],label=["legitimate", "malicious"])

plt.hist([files_legitimate['SectionsMaxEntropy'], files_malware['SectionsMaxEntropy']],color=["green", "blue"],label=["legitimate", "malicious"])

plt.hist([files_legitimate['Characteristics'], files_malware['Characteristics']],color=["green", "blue"],label=["legitimate", "malicious"])

"""Nous avons donc bien la preuve que certaines valeurs sont indispensables à l'analyse des malwares, mais encore une fois, les chercher une par une serait beaucoup trop long. 

Nous avons découvert pendant nos recherches des algorithmes permettant une exploration rapides des informations les plus intéressantes de datasets et permettant de les redimentionner de façon adequate. Ces algorithmes sont présent dans la bibliothèque Scikit de python. 

Dans notre cas, nous allons utiliser le "tree-based feature selection" (toutes les explications relatives à cet algorithme sont disponibles [ici](https://scikit-learn.org/stable/modules/feature_selection.html), section 1.13.4.2)

Nous travaillons sur le dataset complet ici et non plus sur les deux datasets séparés.
"""

isolation = malware

x = isolation.drop(['ID', 'md5', 'legitimate'], axis=1).values
y = isolation['legitimate'].values

Feature_Isolation = ske.ExtraTreesClassifier().fit(x, y)
model = SelectFromModel(Feature_Isolation, prefit=True)
new_X = model.transform(x)

"""Nous pouvons maintenant constater la difference de données présentes dans le dataset original et le dataset filtré:"""

x.shape

new_X.shape

"""Nous sommes passés de 54 à 14. Nous pouvons donc maintenant les afficher et constater les colonnes les plus interessantes:"""

nb_features = new_X.shape[1]

X_train, X_test, y_train, y_test = train_test_split(new_X, y ,test_size=0.2)

indices = np.argsort(Feature_Isolation.feature_importances_)[::-1][:nb_features]

for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, isolation.columns[2+indices[f]], Feature_Isolation.feature_importances_[indices[f]]))

features = []
for f in sorted(np.argsort(Feature_Isolation.feature_importances_)[::-1][:nb_features]):
    features.append(isolation.columns[2+f])

"""Nous constatons que notre idée de depart de travailler avec "SectionsMaxEntropy" s'est avérée juste, en plus d'avoir 13 autres colonnes sur lesquelles travailler.

# Selection de l'algorithme de classification

Dans cette section, nous avons à notre disposition trois algorithmes différents afin de classifier intelligement les binaires.
Afin de selectionner quel algorithme fonctionne le mieux dans notre cas, nous allons utiliser ces 3 algorithmes les un après les autre et comparer leurs resultats. 
Nous allons égalemement rajouter certains algorithmes qui nous semblent pertinents.
"""

algorithms = {
        "Knn": KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform'),
        #"SVM": Pipeline([("scaler", StandardScaler()),("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=10))]),
        "DecisionTree": tree.DecisionTreeClassifier(random_state=0,max_depth=2),
        "RandomForest": ske.RandomForestClassifier(n_estimators=100),
        "GradientBoosting": ske.GradientBoostingClassifier(n_estimators=50),
        "AdaBoost": ske.AdaBoostClassifier(n_estimators=100),
        "GNB": GaussianNB()
    }

results = {}
for algo in algorithms:
    clf = algorithms[algo]
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("%s : %f %%" % (algo, score*100))
    results[algo] = score

winner = max(results, key=results.get)
print('\n Algorithm with highest accuracy on train/test is %s with a %f %% success' % (winner, results[winner]*100))

"""**Remarque**: Nous avons abandonné l'utilisation de l'algorithme SVM à cause de sa durée de calcul, dépassant facilement la dizaine de minutes.

Nous avons donc l'algorithme le plus performant pour notre dataset filtré. Dans notre cas, l'algorithme le plus performant n'est pas un des 3 proposé par le sujet mais bien le RandomForest.

**Pour aller plus loin**: On peut calculer rapidement ici le ratio de faux et de vrais positifs avec notre modèle:
"""

clf = algorithms[winner]
res = clf.predict(X_test)
mt = confusion_matrix(y_test, res)
print("False positive rate : %f %%" % ((mt[0][1] / float(sum(mt[0])))*100))
print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))

"""# Partie 3 : Interface Homme Machine

On veut sauvegarder les resultats afin de les reutiliser pour notre IHM:
"""

for algo in algorithms:
  joblib.dump(algorithms[algo], 'classifier/{0}.pkl'.format(algo))
open('classifier/features_{0}.pkl'.format(algo), 'wb').write(pickle.dumps(features))